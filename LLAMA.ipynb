{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPLdFaD45b3fHRV3iyEMWrc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aartikushal/IC-LLAMA-ASSIGNMENT/blob/main/LLAMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Install dependencies**"
      ],
      "metadata": {
        "id": "KYhquRkVqsl0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zj-6qC7FajUc",
        "outputId": "3b4d9f2d-b919-4ee9-a50f-41f908c43488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "owc1BGRZJRDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Login to Hugging Face**"
      ],
      "metadata": {
        "id": "U4b7x26-q-pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"meta-llama/Llama-2-7b-chat-hf\" # meta-llama/Llama-2-7b-hf\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)"
      ],
      "metadata": {
        "id": "OusaTbeVbuwH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",  # LLM task\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "t-P9vouHcvbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4) Chat helper function**"
      ],
      "metadata": {
        "id": "1Zmx7_H9rT1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_llama_response(prompt: str) -> None:\n",
        "    \"\"\"\n",
        "    Generate a response from the Llama model.\n",
        "\n",
        "    Parameters:\n",
        "        prompt (str): The user's input/question for the model.\n",
        "\n",
        "    Returns:\n",
        "        None: Prints the model's response.\n",
        "    \"\"\"\n",
        "    sequences = llama_pipeline(\n",
        "        prompt,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_length=256,\n",
        "    )\n",
        "    print(\"Chatbot:\", sequences[0]['generated_text'])\n",
        "\n",
        "\n",
        "\n",
        "prompt = 'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n'\n",
        "generate_llama_response(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNCG8l8QcpB1",
        "outputId": "81cbe55a-396c-4201-fb8b-196d8e745502"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
            "\n",
            "Sure! If you enjoyed \"Breaking Bad\" and \"Band of Brothers,\" here are some other shows you might enjoy:\n",
            "\n",
            "1. \"The Sopranos\" - This HBO series is a crime drama that follows the life of a New Jersey mob boss, Tony Soprano, as he navigates the criminal underworld and deals with personal and family issues.\n",
            "2. \"The Wire\" - This HBO series explores the drug trade in Baltimore from multiple perspectives, including law enforcement, drug dealers, and politicians. It's known for its gritty realism and complex characters.\n",
            "3. \"Narcos\" - This Netflix series tells the true story of Pablo Escobar, the infamous Colombian drug lord, and the DEA agents who hunted him down. It's a thrilling and intense drama that explores the world of drug trafficking and organized crime.\n",
            "4. \"Peaky Blinders\" - This BBC series is set in post-World War I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5) Quick test**"
      ],
      "metadata": {
        "id": "PsymfXxir8e6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"I'm a programmer and Python is my favorite language because of it's simple syntax and variety of applications I can build with it.\\\n",
        "Based on that, what language should I learn next?\\\n",
        "Give me 5 recommendations\"\"\"\n",
        "generate_llama_response(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBGwaQAonGmG",
        "outputId": "b0c41ee0-8381-4ecf-e082-25b1e3cbc472"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: I'm a programmer and Python is my favorite language because of it's simple syntax and variety of applications I can build with it.Based on that, what language should I learn next?Give me 5 recommendations and explain why you think they're good choices.\n",
            "\n",
            "1. Java - You should learn Java because it's a popular and versatile language that can be used for developing web applications, Android apps, and enterprise software. Java has a large and active community, which means there are many resources available to help you learn and stay up-to-date with the latest developments. Additionally, Java is known for it's platform independence, which means you can write code that can run on any device that has a Java Virtual Machine (JVM) installed.\n",
            "\n",
            "2. JavaScript - JavaScript is a must-know language for any web developer. It's used to create interactive web pages, dynamic user interfaces, and web applications. With the rise of front-end development, JavaScript has become even more important, and it's now used in a wide range of applications, from mobile app development to server-side programming. Additionally, JavaScript has a large and active community, which means there are many resources available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'How to do quick breakfast using non startchy veggies?\\n'\n",
        "generate_llama_response(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcnAXcmznO5J",
        "outputId": "74aee871-aed4-4eb5-c62f-8bd996d457b3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: How to do quick breakfast using non startchy veggies?\n",
            "Here are some quick breakfast ideas using non-starchy vegetables:\n",
            "\n",
            "1. Spinach and feta omelette: Whisk together 2 eggs, 1/4 cup fresh spinach leaves, and 1/4 cup crumbled feta cheese. Heat a non-stick pan over medium heat, add the eggs and cook until set. Fold in half and serve.\n",
            "2. Avocado toast: Toast whole grain bread, mash 1/2 avocado and spread on top. Add a squeeze of lemon juice and a sprinkle of salt and pepper to taste.\n",
            "3. Broccoli and cheese breakfast burrito: Scramble 2 eggs, add 1/4 cup chopped broccoli florets and 1/4 cup shredded cheese. Wrap in a whole grain tortilla and microwave for 30-45 seconds or until the cheese is melted.\n",
            "4. Carrot and orange breakfast smoothie: Combine 1/2 cup frozen carrots, 1/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"bye\", \"quit\", \"exit\"]:\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "    generate_llama_response(user_input)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV0z2RpspvV9",
        "outputId": "f53955a4-e553-42a2-b76a-91b16f2ccc1a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: who is pm of india\n",
            "Chatbot: who is pm of india\n",
            "\n",
            "The Prime Minister of India is the head of government of India. The Prime Minister is responsible for leading the country and implementing the policies of the Government of India. The Prime Minister is also the ex-officio chairman of the Indian National Congress and the leader of the Council of Ministers.\n",
            "\n",
            "The current Prime Minister of India is Narendra Modi, who took office on May 26, 2014. Modi is the first Prime Minister to have been born after India gained independence from British colonial rule. He was re-elected for a second term in the 2019 Indian general election.\n",
            "\n",
            "The Prime Minister is appointed by the President of India, who is elected indirectly by the members of the Parliament. The Prime Minister is usually the leader of the political party or coalition that has a majority of seats in the Lok Sabha, the lower house of the Indian Parliament.\n",
            "\n",
            "The Prime Minister has a number of important powers and responsibilities, including:\n",
            "\n",
            "1. Head of Government: The Prime Minister is the head of the government and is responsible for leading the country and implementing the policies of the government.\n",
            "2. Appointment of Ministers\n",
            "You: exit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6) Gradio Chat UI**"
      ],
      "metadata": {
        "id": "yll0vRr3vgxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SYSTEM_PROMPT = \"You are a helpful, concise assistant.\"\n",
        "MAX_TOKENS = 512\n",
        "TEMPERATURE = 0.7\n",
        "TOP_P = 0.95\n",
        "\n",
        "def format_prompt(history, user_message):\n",
        "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "    for u, a in history:\n",
        "        if u:\n",
        "            messages.append({\"role\": \"user\", \"content\": u})\n",
        "        if a:\n",
        "            messages.append({\"role\": \"assistant\", \"content\": a})\n",
        "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    # Build chat prompt in LLaMA-2 format\n",
        "    prompt = \"\"\n",
        "    for m in messages:\n",
        "        if m[\"role\"] == \"system\":\n",
        "            prompt += f\"<s>[INST] <<SYS>>\\n{m['content']}\\n<</SYS>>\\n\"\n",
        "        elif m[\"role\"] == \"user\":\n",
        "            prompt += f\"{m['content']} [/INST] \"\n",
        "        elif m[\"role\"] == \"assistant\":\n",
        "            prompt += f\"{m['content']} </s><s>[INST] \"\n",
        "    return prompt\n",
        "\n",
        "def generate_chat_response(history, user_message):\n",
        "    prompt = format_prompt(history, user_message)\n",
        "    out = llama_pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=MAX_TOKENS,\n",
        "        do_sample=True,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    reply = out[0][\"generated_text\"][len(prompt):]\n",
        "    return reply.strip()"
      ],
      "metadata": {
        "id": "-QHOylEtI1EK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# 🦙 LLaMA-2 7B Chatbot\")\n",
        "    chatbot = gr.Chatbot(height=400)\n",
        "    with gr.Row():\n",
        "        msg = gr.Textbox(placeholder=\"Type a message...\", scale=8)\n",
        "        clear = gr.Button(\"Clear\", scale=1)\n",
        "    temp = gr.Slider(0.0, 1.5, value=TEMPERATURE, step=0.05, label=\"Temperature\")\n",
        "    top_p = gr.Slider(0.1, 1.0, value=TOP_P, step=0.05, label=\"Top-p\")\n",
        "    max_new = gr.Slider(64, 1024, value=MAX_TOKENS, step=32, label=\"Max new tokens\")\n",
        "\n",
        "    def respond(user_message, chat_history, temperature, top_p, max_new_tokens):\n",
        "        global TEMPERATURE, TOP_P, MAX_TOKENS\n",
        "        TEMPERATURE = float(temperature)\n",
        "        TOP_P = float(top_p)\n",
        "        MAX_TOKENS = int(max_new_tokens)\n",
        "        bot_message = generate_chat_response(chat_history, user_message)\n",
        "        chat_history = chat_history + [(user_message, bot_message)]\n",
        "        return \"\", chat_history\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot, temp, top_p, max_new], [msg, chatbot])\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "Rd__IEMZJ9d2",
        "outputId": "d8d3a9a4-13ea-4c3e-db2f-1b71500b1279"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4037629385.py:5: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(height=400)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://af4c18a3e0f0522d49.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://af4c18a3e0f0522d49.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}